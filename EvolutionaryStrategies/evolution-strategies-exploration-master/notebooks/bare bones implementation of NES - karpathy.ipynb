{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0. w: [ 1.76405235  0.40015721  0.97873798], solution: [ 0.5  0.1 -0.3], reward: -3.323094\n",
      "iter 20. w: [ 1.63796944  0.36987244  0.84497941], solution: [ 0.5  0.1 -0.3], reward: -2.678783\n",
      "iter 40. w: [ 1.50042904  0.33577052  0.70329169], solution: [ 0.5  0.1 -0.3], reward: -2.063040\n",
      "iter 60. w: [ 1.36438269  0.29247833  0.56990397], solution: [ 0.5  0.1 -0.3], reward: -1.540938\n",
      "iter 80. w: [ 1.2257328   0.25622233  0.43607161], solution: [ 0.5  0.1 -0.3], reward: -1.092895\n",
      "iter 100. w: [ 1.08819889  0.22827364  0.30415088], solution: [ 0.5  0.1 -0.3], reward: -0.727430\n",
      "iter 120. w: [ 0.95675286  0.19282042  0.16682465], solution: [ 0.5  0.1 -0.3], reward: -0.435164\n",
      "iter 140. w: [ 0.82214521  0.16161165  0.03600742], solution: [ 0.5  0.1 -0.3], reward: -0.220475\n",
      "iter 160. w: [ 0.70282088  0.12935569 -0.09779598], solution: [ 0.5  0.1 -0.3], reward: -0.082885\n",
      "iter 180. w: [ 0.58380424  0.11579811 -0.21083135], solution: [ 0.5  0.1 -0.3], reward: -0.015224\n",
      "iter 200. w: [ 0.52089064  0.09897718 -0.2761225 ], solution: [ 0.5  0.1 -0.3], reward: -0.001008\n",
      "iter 220. w: [ 0.50861791  0.10220363 -0.29023563], solution: [ 0.5  0.1 -0.3], reward: -0.000174\n",
      "iter 240. w: [ 0.50428202  0.10834192 -0.29828744], solution: [ 0.5  0.1 -0.3], reward: -0.000091\n",
      "iter 260. w: [ 0.50147991  0.1044559  -0.30255291], solution: [ 0.5  0.1 -0.3], reward: -0.000029\n",
      "iter 280. w: [ 0.50208135  0.0986722  -0.29841024], solution: [ 0.5  0.1 -0.3], reward: -0.000009\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A bare bones examples of optimizing a black-box function (f) using\n",
    "Natural Evolution Strategies (NES), where the parameter distribution is a \n",
    "gaussian of fixed standard deviation.\n",
    "\n",
    "source: https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# the function we want to optimize\n",
    "def f(w):\n",
    "  # here we would normally:\n",
    "  # ... 1) create a neural network with weights w\n",
    "  # ... 2) run the neural network on the environment for some time\n",
    "  # ... 3) sum up and return the total reward\n",
    "\n",
    "  # but for the purposes of an example, lets try to minimize\n",
    "  # the L2 distance to a specific solution vector. So the highest reward\n",
    "  # we can achieve is 0, when the vector w is exactly equal to solution\n",
    "  reward = -np.sum(np.square(solution - w))\n",
    "  return reward\n",
    "\n",
    "# hyperparameters\n",
    "npop = 50 # population size\n",
    "sigma = 0.1 # noise standard deviation\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "# start the optimization\n",
    "solution = np.array([0.5, 0.1, -0.3])\n",
    "w = np.random.randn(3) # our initial guess is random\n",
    "for i in range(300):\n",
    "\n",
    "  # print current fitness of the most likely parameter setting\n",
    "  if i % 20 == 0:\n",
    "    print('iter %d. w: %s, solution: %s, reward: %f' % \n",
    "          (i, str(w), str(solution), f(w)))\n",
    "\n",
    "  # initialize memory for a population of w's, and their rewards\n",
    "  N = np.random.randn(npop, 3) # samples from a normal distribution N(0,1)\n",
    "  R = np.zeros(npop)\n",
    "  for j in range(npop):\n",
    "    w_try = w + sigma*N[j] # jitter w using gaussian of sigma 0.1\n",
    "    R[j] = f(w_try) # evaluate the jittered version\n",
    "\n",
    "  # standardize the rewards to have a gaussian distribution\n",
    "  A = (R - np.mean(R)) / np.std(R)\n",
    "  # perform the parameter update. The matrix multiply below\n",
    "  # is just an efficient way to sum up all the rows of the noise matrix N,\n",
    "  # where each row N[j] is weighted by A[j]\n",
    "  w = w + alpha/(npop*sigma) * np.dot(N.T, A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
